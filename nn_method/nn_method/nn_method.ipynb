{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_method.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtCKSTUE1rcp",
        "colab_type": "code",
        "outputId": "ca3e33bc-b7db-4403-f476-e1e73609c117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cki3Jvx_5d89",
        "colab_type": "code",
        "outputId": "7839314b-8ab3-49e1-9ab1-c64b5776e7c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import sys                                                                      #\n",
        "sys.path.append('/content/drive/My Drive/nn_method')                                  #\n",
        "!pip install wikipedia                                                          #\n",
        "!pip install unidecode                                                          #\n",
        "import numpy as np                                                              #\n",
        "from keras.models import Sequential                                             #\n",
        "from keras.layers import Dense                                                  #\n",
        "from keras.layers import Embedding                                              #             Importing necessary packages\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard                        #\n",
        "from sklearn.model_selection import train_test_split                            #\n",
        "from config import language_tags, max_letters                                   #\n",
        "import random                                                                   #\n",
        "from itertools import chain                                                     #\n",
        "from functions import convert_dic_to_vector , process                           #"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.9.11)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_IxibVO5d_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.load('/content/drive/My Drive/nn_method/arr.npy','r')\n",
        "# print(len(language_tags))\n",
        "inputs = data[:, 4:]                                                 #       splitting data into inputs and labels\n",
        "labels = data[:, 1:4]                                                #       \n",
        "                                                                                        #\n",
        "x_train, x_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.15)     #       splitting data into into test and validation sets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQvzRKYd5eDx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "46d56831-07d6-4a97-c2bf-20901a918cfa"
      },
      "source": [
        "network = Sequential()                                                                  #\n",
        "network.add(Dense(200, input_dim=26*max_letters, activation='relu'))                    #\n",
        "network.add(Dense(150, activation='relu'))                                              #\n",
        "network.add(Dense(100, activation='relu'))                                              #           Defining Model\n",
        "network.add(Dense(100, activation='relu'))                                              #\n",
        "network.add(Dense(3, activation='softmax'))                            #\n",
        "                                                                                        #\n",
        "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])     #"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhgc6Dku5eHq",
        "colab_type": "code",
        "outputId": "b292e126-afe8-4540-d3dc-4e2eb4e402d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "filepath = \"weights.hdf5\"                                                                                   #   \n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')       #\n",
        "tboard = TensorBoard(log_dir='./logs', write_graph=True, write_images=True)                               #           Model fitting\n",
        "callbacks_list = [checkpoint, tboard]                                                                     #\n",
        "                                                                                                          #\n",
        "network.fit(x_train, y_train, epochs=50, batch_size=1000, validation_data=(x_test, y_test), callbacks=callbacks_list)   #"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 32471 samples, validate on 5731 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/50\n",
            "32471/32471 [==============================] - 11s 337us/step - loss: 0.5309 - acc: 0.7318 - val_loss: 0.3553 - val_acc: 0.8487\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.84866, saving model to weights.hdf5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/50\n",
            "32471/32471 [==============================] - 10s 305us/step - loss: 0.2972 - acc: 0.8787 - val_loss: 0.2486 - val_acc: 0.8971\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.84866 to 0.89711, saving model to weights.hdf5\n",
            "Epoch 3/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.2024 - acc: 0.9214 - val_loss: 0.1998 - val_acc: 0.9168\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.89711 to 0.91683, saving model to weights.hdf5\n",
            "Epoch 4/50\n",
            "32471/32471 [==============================] - 10s 305us/step - loss: 0.1541 - acc: 0.9402 - val_loss: 0.1736 - val_acc: 0.9331\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.91683 to 0.93305, saving model to weights.hdf5\n",
            "Epoch 5/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.1261 - acc: 0.9511 - val_loss: 0.1651 - val_acc: 0.9387\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.93305 to 0.93870, saving model to weights.hdf5\n",
            "Epoch 6/50\n",
            "32471/32471 [==============================] - 10s 310us/step - loss: 0.1085 - acc: 0.9581 - val_loss: 0.1608 - val_acc: 0.9428\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.93870 to 0.94277, saving model to weights.hdf5\n",
            "Epoch 7/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0931 - acc: 0.9635 - val_loss: 0.1604 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.94277 to 0.94527, saving model to weights.hdf5\n",
            "Epoch 8/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0859 - acc: 0.9659 - val_loss: 0.1652 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.94527 to 0.94527, saving model to weights.hdf5\n",
            "Epoch 9/50\n",
            "32471/32471 [==============================] - 10s 310us/step - loss: 0.0771 - acc: 0.9707 - val_loss: 0.1660 - val_acc: 0.9471\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.94527 to 0.94707, saving model to weights.hdf5\n",
            "Epoch 10/50\n",
            "32471/32471 [==============================] - 10s 309us/step - loss: 0.0740 - acc: 0.9713 - val_loss: 0.1646 - val_acc: 0.9476\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.94707 to 0.94759, saving model to weights.hdf5\n",
            "Epoch 11/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0681 - acc: 0.9732 - val_loss: 0.1756 - val_acc: 0.9457\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.94759\n",
            "Epoch 12/50\n",
            "32471/32471 [==============================] - 10s 309us/step - loss: 0.0684 - acc: 0.9732 - val_loss: 0.1747 - val_acc: 0.9459\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.94759\n",
            "Epoch 13/50\n",
            "32471/32471 [==============================] - 10s 309us/step - loss: 0.0642 - acc: 0.9747 - val_loss: 0.1696 - val_acc: 0.9475\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.94759\n",
            "Epoch 14/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0632 - acc: 0.9752 - val_loss: 0.1732 - val_acc: 0.9451\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.94759\n",
            "Epoch 15/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0625 - acc: 0.9751 - val_loss: 0.1713 - val_acc: 0.9475\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.94759\n",
            "Epoch 16/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0608 - acc: 0.9757 - val_loss: 0.1754 - val_acc: 0.9501\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.94759 to 0.95010, saving model to weights.hdf5\n",
            "Epoch 17/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0592 - acc: 0.9758 - val_loss: 0.1751 - val_acc: 0.9464\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.95010\n",
            "Epoch 18/50\n",
            "32471/32471 [==============================] - 10s 308us/step - loss: 0.0585 - acc: 0.9756 - val_loss: 0.1782 - val_acc: 0.9463\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.95010\n",
            "Epoch 19/50\n",
            "32471/32471 [==============================] - 10s 309us/step - loss: 0.0580 - acc: 0.9768 - val_loss: 0.1726 - val_acc: 0.9494\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.95010\n",
            "Epoch 20/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0558 - acc: 0.9775 - val_loss: 0.1792 - val_acc: 0.9495\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.95010\n",
            "Epoch 21/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0572 - acc: 0.9768 - val_loss: 0.1802 - val_acc: 0.9491\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.95010\n",
            "Epoch 22/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0556 - acc: 0.9779 - val_loss: 0.1755 - val_acc: 0.9481\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.95010\n",
            "Epoch 23/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0542 - acc: 0.9773 - val_loss: 0.1779 - val_acc: 0.9491\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.95010\n",
            "Epoch 24/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0542 - acc: 0.9776 - val_loss: 0.1798 - val_acc: 0.9475\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.95010\n",
            "Epoch 25/50\n",
            "32471/32471 [==============================] - 10s 310us/step - loss: 0.0542 - acc: 0.9778 - val_loss: 0.1771 - val_acc: 0.9498\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.95010\n",
            "Epoch 26/50\n",
            "32471/32471 [==============================] - 10s 305us/step - loss: 0.0542 - acc: 0.9776 - val_loss: 0.1776 - val_acc: 0.9490\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.95010\n",
            "Epoch 27/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0535 - acc: 0.9773 - val_loss: 0.1753 - val_acc: 0.9504\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.95010 to 0.95044, saving model to weights.hdf5\n",
            "Epoch 28/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0547 - acc: 0.9774 - val_loss: 0.1754 - val_acc: 0.9508\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.95044 to 0.95079, saving model to weights.hdf5\n",
            "Epoch 29/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0525 - acc: 0.9778 - val_loss: 0.1725 - val_acc: 0.9518\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.95079 to 0.95184, saving model to weights.hdf5\n",
            "Epoch 30/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0507 - acc: 0.9786 - val_loss: 0.1780 - val_acc: 0.9508\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.95184\n",
            "Epoch 31/50\n",
            "32471/32471 [==============================] - 10s 310us/step - loss: 0.0511 - acc: 0.9777 - val_loss: 0.1737 - val_acc: 0.9511\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.95184\n",
            "Epoch 32/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0501 - acc: 0.9781 - val_loss: 0.1857 - val_acc: 0.9486\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.95184\n",
            "Epoch 33/50\n",
            "32471/32471 [==============================] - 10s 308us/step - loss: 0.0514 - acc: 0.9781 - val_loss: 0.1889 - val_acc: 0.9459\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.95184\n",
            "Epoch 34/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0516 - acc: 0.9770 - val_loss: 0.1799 - val_acc: 0.9506\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.95184\n",
            "Epoch 35/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0513 - acc: 0.9770 - val_loss: 0.1811 - val_acc: 0.9496\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.95184\n",
            "Epoch 36/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0505 - acc: 0.9782 - val_loss: 0.1811 - val_acc: 0.9502\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.95184\n",
            "Epoch 37/50\n",
            "32471/32471 [==============================] - 10s 310us/step - loss: 0.0510 - acc: 0.9780 - val_loss: 0.1818 - val_acc: 0.9489\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.95184\n",
            "Epoch 38/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0503 - acc: 0.9789 - val_loss: 0.1811 - val_acc: 0.9495\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.95184\n",
            "Epoch 39/50\n",
            "32471/32471 [==============================] - 10s 308us/step - loss: 0.0493 - acc: 0.9793 - val_loss: 0.1780 - val_acc: 0.9502\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.95184\n",
            "Epoch 40/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0483 - acc: 0.9782 - val_loss: 0.1804 - val_acc: 0.9514\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.95184\n",
            "Epoch 41/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0493 - acc: 0.9783 - val_loss: 0.1853 - val_acc: 0.9495\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.95184\n",
            "Epoch 42/50\n",
            "32471/32471 [==============================] - 10s 307us/step - loss: 0.0491 - acc: 0.9780 - val_loss: 0.1827 - val_acc: 0.9496\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.95184\n",
            "Epoch 43/50\n",
            "32471/32471 [==============================] - 10s 311us/step - loss: 0.0493 - acc: 0.9786 - val_loss: 0.1794 - val_acc: 0.9515\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.95184\n",
            "Epoch 44/50\n",
            "32471/32471 [==============================] - 10s 308us/step - loss: 0.0491 - acc: 0.9780 - val_loss: 0.1786 - val_acc: 0.9497\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.95184\n",
            "Epoch 45/50\n",
            "32471/32471 [==============================] - 10s 308us/step - loss: 0.0486 - acc: 0.9789 - val_loss: 0.1792 - val_acc: 0.9517\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.95184\n",
            "Epoch 46/50\n",
            "32471/32471 [==============================] - 10s 308us/step - loss: 0.0488 - acc: 0.9775 - val_loss: 0.1842 - val_acc: 0.9497\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.95184\n",
            "Epoch 47/50\n",
            "32471/32471 [==============================] - 10s 306us/step - loss: 0.0482 - acc: 0.9783 - val_loss: 0.1845 - val_acc: 0.9505\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.95184\n",
            "Epoch 48/50\n",
            "32471/32471 [==============================] - 10s 308us/step - loss: 0.0481 - acc: 0.9784 - val_loss: 0.1843 - val_acc: 0.9493\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.95184\n",
            "Epoch 49/50\n",
            "32471/32471 [==============================] - 10s 310us/step - loss: 0.0481 - acc: 0.9790 - val_loss: 0.1852 - val_acc: 0.9509\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.95184\n",
            "Epoch 50/50\n",
            "32471/32471 [==============================] - 10s 305us/step - loss: 0.0493 - acc: 0.9779 - val_loss: 0.1827 - val_acc: 0.9507\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.95184\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a2ec280b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duDlDL8Y5eKF",
        "colab_type": "code",
        "outputId": "3ff8bb7b-ef1d-4034-dd41-1151e6073121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "file1 = open('/content/drive/My Drive/nn_method/test.txt', 'r')                       # \n",
        "Lines = file1.readlines()                                                       #\n",
        "punctuations = \".,!?;'/\\n\"                                                      #   Opening test file. \n",
        "no_punct = \"\"                                                                   #   Model reports the language of this test file\n",
        "alpha=[]                                                                        #\n",
        "\n",
        "for line in Lines:                                                              #\n",
        "    temp=line.lower()                                                           #\n",
        "    for char in temp:                                                           #   \n",
        "        if char in punctuations:                                                #     Processing test.txt file\n",
        "            temp=temp.replace(char,\"\")                                          #\n",
        "    alpha.append(temp)                                                          #\n",
        "\n",
        "alpha1=[]                                                                       #\n",
        "for j in range (len(alpha)):                                                    #\n",
        "  alpha1.append(process(alpha[j],200))                                          #\n",
        "\n",
        "\n",
        "alpha1 = list(chain.from_iterable(alpha1))  \n",
        "print(alpha1)                                                                   #   printing words obtained\n",
        "\n",
        "network = Sequential()                                                          #\n",
        "network.add(Dense(200, input_dim=26*max_letters, activation='relu'))            #\n",
        "network.add(Dense(150, activation='relu'))                                      #\n",
        "network.add(Dense(100, activation='relu'))                                      #       Loading pre-trained model with weights\n",
        "network.add(Dense(100, activation='relu'))                                      #\n",
        "network.add(Dense(3, activation='softmax'))                                     #\n",
        "network.load_weights('weights.hdf5')                                            #\n",
        "network.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy']) #\n",
        "\n",
        "temp0=np.zeros((1,3))                                                           #\n",
        "\n",
        "dic = []                                                                        #         \n",
        "for word in alpha1 :                                                            #\n",
        "  dic.append(word)                                                              #   Converting the processed string to vector\n",
        "  vct_str = convert_dic_to_vector(dic, max_letters)                             #  \n",
        "  vct = np.zeros((1, 26 * max_letters))                                         #\n",
        "  count = 0                                                                       #\n",
        "\n",
        "\n",
        "  for digit in vct_str[0]:                                                        #\n",
        "      vct[0,count] = int(digit)                                                 #\n",
        "      count += 1                                                                #\n",
        "  prediction_vct = network.predict(vct)                                         #    \n",
        "  temp0+=prediction_vct                                                         #     Summing individual probabilities for each words\n",
        "                                                                                \n",
        "\n",
        "t=np.argmax(temp0[0])                                                           #   \n",
        "\n",
        "if t==0:                                                                        #\n",
        "  print(\"English\")                                                              #\n",
        "if t==1 :                                                                       #\n",
        "  print(\"German\")                                                               #       Reporting Language with cumulative highest probability\n",
        "if t==2 :                                                                       #\n",
        "  print(\"French\")                                                               #                                                                   #"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['poems', 'awaken', 'the', 'dormant', 'soul', 'in', 'us', 'we', 'all', 'come', 'at', 'place', 'where', 'we', 'feel', 'dejected', 'and', 'disenfranchised', 'with', 'this', 'materialistic', 'world', 'once', 'in', 'our', 'lives', 'at', 'that', 'time', 'of', 'undetermined', 'condition', 'and', 'confusion', 'the', 'poems', 'will', 'make', 'our', 'heart', 'clear', 'of', 'all', 'the', 'debris', 'and', 'unwanted', 'feelings', 'and', 'make', 'our', 'heart', 'pure', 'with', 'the', 'awakened', 'soul', 'we', 'have', 'to', 'make', 'it', 'clear', 'that', 'we', 'want', 'peace', 'and', 'love', 'in', 'this', 'wonderful', 'life', 'over', 'all', 'other', 'things', 'of', 'materialism', 'here', 'are', 'those', 'poems', 'which', 'come', 'under', 'this', 'kind', 'of', 'nature', 'to', 'make', 'yourself', 'proud', 'of', 'having', 'been', 'born', 'man', 'woman', 'in', 'this', 'beautiful', 'world', 'we', 'have', 'collected', 'these', 'poems', 'with', 'deligence', 'and', 'longwaiting', 'the', 'list', 'may', 'vary', 'from', 'person', 'to', 'person', 'of', 'course', 'no', 'one', 'could', 'determine', 'what', 'the', 'best', 'poem', 'is', 'the', 'choice', 'of', 'yours', 'may', 'be', 'different', 'ours', 'is', 'not', 'the', 'fianl', 'list', 'you', 'can', 'prepare', 'your', 'own', 'list', 'of', 'the', 'top', 'poems']\n",
            "French\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUB2LlRM7iGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}